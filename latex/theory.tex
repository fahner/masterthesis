\chapter{Theory and motivation}
\label{sec:Theory}

\section{The Standard Model of Particle Physics}
The Standard Model of particle physics (SM) is a relativistic and renormalisable quantum field theory, that combines the electroweak theory developed by Glashow, Salam and Weinberg \cite{SM_Glashow, SM_Salam, SM_Weinberg} with quantum chromodynamics (QCD), the theory of the strong interaction.
It combines the current knowledge of fundamental particles and their interactions at microscopic level with the exception of gravitation.

The electroweak theory is already a combination of the electromagnetic and the weak interaction.
Every fundamental particle can interact via the weak force.
For instance, the weak force is responsible for the $\beta$-decay of neutrons.
Atoms or molecules are bound by the electromagnetic interaction taking place between electrically charged particles.
If particles also carry a so-called colour charge, they can interact via the strong interaction, which binds e.g. protons and neutrons in a nucleus.

In the Standard Model, matter arises as half-integer spin particles from quantum fields.
These particles are called \textsc{Fermions} and can be furthermore split up in \textsc{Quarks} and \textsc{Leptons}.
There exist 6 so-called ``flavours" of quarks: up (\uquark), down (\cquark), charm (\cquark), strange (\squark), top (\tquark) and bottom\footnote{The bottom quark is sometimes also called beauty.} (\bquark).
According to their mass and other physical properties, they can be grouped into three generations:
\begin{align*}
    \begin{pmatrix} \uquark \\ \dquark \end{pmatrix},
    \begin{pmatrix} \cquark \\ \squark \end{pmatrix},
    \begin{pmatrix} \tquark \\ \bquark \end{pmatrix}.
\end{align*}
Quarks in the top row are referred to as \textsc{Uptype} quark.
They carry an electrical charge of $+\frac{2}{3}e$, whereas \textsc{Downtype} quarks carry a charge of $-\frac{1}{3}e$.
Thus, they can participate in the electromagnetic interaction.
In addtion, quarks carry colour charge allowing them to interact via the strong interaction.

Leptons do not carry flavour as opposed to quarks.
There exist three flavours of leptons, namely the electron (\en), the muon (\mun) and the tauon (\taum) with an electric charge of $-e$ and their neutral counterparts, the neutrinos \neue, \neum, \neut.
Similarly to quarks, the leptons can be grouped into three generations
\begin{align*}
    \begin{pmatrix} \en \\ \neue \end{pmatrix},
    \begin{pmatrix} \mun \\ \neum \end{pmatrix},
    \begin{pmatrix} \taum \\ \neut \end{pmatrix}.
\end{align*}
As the neutrinos do neither carry electrical nor colour charge, they only interact weakly.
Due to that fact it is not possible to detect neutrinos at hadron colliders like the \lhc.
For each fermion, there exists a corresponding antifermion with same mass, but opposite quantum numbers.

In the Standard Model, the interactions among the particles are described by the mediation of spin-1 gauge bosons.
For the electromagnetic interaction, this is the electric neutral photon \g.
It couples to electrically charged particles.
Since the photon is massless, the range of the electromagnetic interaction is infinite.
The weak interaction is mediated by three massive gauge bosons:
The electrically charged \Wp and \Wm as well as the neutral \Z boson.
The \Wpm bosons only couple to left-handed fermions (or right-handed antifermions), whereas the \Z couples to both, left- and right-handed fermions, but with different strength.
Due to their large mass of $m_{\Wpm} \approx 80 \gev$ and $m_\Z \approx 91 \gev$ the weak interaction is only short-ranged.
The strong interaction is carried by 8 massless gluons.
They couple to particles carrying a colour charge and carry colour itself.
Thus, there is also a gluon-gluon coupling possible leading to a QCD coupling strength \as, which strongly depends on the momentum transfer in an interaction.
For low energies, \as increases dramatically, which means that coloured particles cannot be isolated.
This phenomenon is called \textsc{Confinement}.
At high energies, \as is very small resulting in the \textsc{Asymptotic Freedom}, i.e. the quarks are thus quasi-free while keeping short distances.
Due to the confinement, \textsc{Hadrons}, strongly interacting composite particles, must always be colour-neutral.
They exist either as quark-antiquark system and are called \textsc{Mesons} or as composite of three quarks and are named \textsc{Baryons}.
Recent \lhcb measurements report the observation of candidates for bound states consisting of even 4 quarks (2 quark, 2 antiquarks) \cite{Tetraquark} and also 5 quarks (4 quarks, 1 antiquark) \cite{Pentaquark}, called Tetraquarks or Pentaquarks respectively.

Contradicting to the properties of the particles mentioned above, the invariance of local gauge transformation requires that the particles of the Standard Model are massless.
This problem is solved by the \textsc{Higgs mechanism}, which introduces a doublet of complex scalar (spin 0) fields.
The potential of these fields spontaneously break the electroweak symmetry and leads to massive bosons and fermions due to interaction with the Higgs field.
The Higgs mechanism furthermore predicts a massive spin-0 particle, the \textsc{Higgs Boson}.
As lastly unobserved particle, its discovery in July 2012 by \atlas \cite{Higgs_ATLAS} and \cms \cite{Higgs_CMS} was a big success for the experimental community as well as for the theory of the Standard Model itself. 
Figure \ref{fig:SM} summarises the fermions and bosons of the Standard Model and lists their main properties \cite{Perkins_HEP, Burgess_SM, Meissner}.
\begin{figure}[tb]
    \centering
	\includegraphics[width=\textwidth]{Standard_Model}	
	\caption{Summary of all fundamental fermions and bosons of the Standard Model of particle physics with their most important properties. Figure slightly modified and taken from \cite{SM_figure}.}
	\label{fig:SM}
\end{figure}

\section{The interest in \LbToDpmunuX}
There are several reasons why the study of the decay \LbToDpmunuX, which is done in this thesis, is interesting.
On the one hand the \Dz\proton subsytem allows spectroscopical studies to learn more about QCD, on the other hand the decay is a background in studies sensitive to physics beyond the Standard Model and thus limiting their precision.
The following sections ought to briefly introduce the areas and studies, where a better understanding of \LbToDpmunuX is helpful.

Before, the decay \LbToDpmunuX itself is introduced.
The \Lb is a baryon with the quark content \uquark\dquark\bquark and a mass of $(5619.5 \pm 0.4)\mev$ \cite{PDG}.
Among all \bquark baryons it is the lightest one.
Hence, it can only decay weakly as the strong and the electromagnetic interaction forbid quark transitions into other flavours.
This results in a characteristic, relatively long lifetime of $(1.451 \pm 0.013)\ps$ \cite{PDG}, which is helpful for the detection and reconstruction as will be explained later.
The \bquark quark decays into a \cquark by the emission of a \Wm boson.
Subsequently, the \Wm boson ends up in a muon \mun and a neutrino \neumb.
With a \uubar pair from the vacuum it is then possible to form a proton \proton (quark content: \uquark\uquark\dquark) and a \Dz meson (\cquark\uquarkbar) in the final state.
Figure \ref{fig:LbToDpmunu} shows a possible Feynman graph for the decay \LbToDpmunu.
For the analysis later it is important to mention, that the mass of the \Dz meson is $(1864.84 \pm 0.07)\mev$ and of the proton\footnote{The uncertainty on the proton is negligible compared to the other particles in this analysis.} $938.27\mev$ \cite{PDG}.
Thus, a particle decaying into a \Dz\proton must have a mass above the \Dz\proton mass threshold of about $2803\mev$.
\begin{figure}[tb]
    \centering
	\includegraphics[width=\textwidth]{LbToD0pmunu}	
	\caption{Feynman diagram for the decay \LbToDpmunu.}
	\label{fig:LbToDpmunu}
\end{figure}

\subsection{Baryonic spectroscopy}
There are plenty of combinatoric possibilities to combine three quarks to a baryon.
The constituent quark model predicts seven ground-state baryons with a total angular momentum $J$ and parity $P$ of $J^P = \frac{1}{2}^+$, containing a heavy \bquark quark and two light (\uquark, \dquark or \squark) quarks: 
The \Lb singlet, the $\Sigma_b$ triplet, the $\Xi_b$ doublet and the \Omegab \cite{LHCb_Dph}.
Except for the $\Sigma_b^0$, all these states have been observed.
However the current knowledge of their fundamental properties like masses, widths and quantum numbers is poor as well as only few decay channels have been measured.
Thus, there is a big interest in the study of \bquark baryons.

With the transition from a \bquark to a \cquark by the emission of a \Wm boson, the \Lb can decay into a \Lc baryon.
The quark content of the \Lc is \uquark\dquark\cquark and thus a representative of a rich spectrum of \textsc{charmed baryons}.
Figure \ref{fig:Lc_spectrum} shows all known charmed baryons with their quantum number $J^P$ and their observed decays.
\begin{figure}[tb]
    \centering
	\includegraphics[width=0.7\textwidth]{Lc_spectrum}	
	\caption{Summary of all known and established charmed baryons with quantum numbers $J^P$ and their observed decays. Figure taken from \cite{PDG}.}
	\label{fig:Lc_spectrum}
\end{figure}
Again, this spectrum is not complete.
A lot of expected charmed baryons are not established and also quantum numbers etc. of established charmed baryons have not been measured so far.
In 2006, \babar reported the observation of the charmed baryon decays \decay{\LcResI}{\Dz\proton} and \decay{\LcResII}{\Dz\proton} \cite{BaBar_D0p}.
Thus the decay \LbToDpmunuX, studied in this thesis offers the possibility to study \Lc resonances above the \Dz\proton mass threshold of $2803\mev$ by investigating the \Dz\proton subsystem.

The charmed baryon spectrum in Figure \ref{fig:Lc_spectrum} reminds of the emission spectrum of a hydrogen atom.
While the electromagnetic force is responsible for the splitting of the hydrogen's energy levels, the spectroscopy of baryons offers the possibility to better understand QCD, the theory of the strong interaction.
Thus, QCD should be able to describe the dynamics of the baryons and predict their masses.
However, due to the rising coupling strength \as at short distances, QCD cannot be treated perturbatively in the context of baryons.
Thus, there exist different approaches trying to solve QCD problems not generally, but at least under certain conditions.
Examples for such approaches are Lattice QCD \cite{LatticeQCD} or effective theories like the Heavy Quark Effective Theory (HQET) \cite{HQET_Introduction}.
The latter one has proven to be useful by the description of $b$ hadrons.
The main principle is that one considers the heavy $b$ quark as static source of the gluon field surrounded by the light quarks.
This theoretical simplification in describing the dynamics is analogous to consider a hydrogen atom instead of a positronium \cite{HQET_Introduction}.

\subsection{The hunt for New Physics at \lhcb}
It has already been stated in the introduction, that the Standard Model does not cover and explain all observed phenomena.
For instance, gravitation is not included in the Standard model, it does not provide a candidate for dark matter as well as dark energy and the asymmetry between matter and antimatter in the Standard Model is not large enough to fully explain why matter and animatter have not fully annihilated after
the big bang.
All physics that go beyond the Standard Model and try to answer those questions are generally referred to as \textsc{New Physics (NP)}.

In principle, there are two ways to discover New Physics:
The first one is to directly search for new particles.
This is what \atlas and \cms are doing and what lead to the discovery of the Higgs boson in 2012 \cite{Higgs_ATLAS, Higgs_CMS}.
However, one can only discover particles with a mass less than the available centre-of-mass energy \sqs.

The \lhcb way of searching for New Physics is an indirect one by the investigation of loop processes such as the \B/\Bbar mixing\footnote{A brief introduction of meson mixing will follow in the next sections.} shown in Figure \ref{fig:Bmixing}.
The virtual particles in the loops do not need to satisfy Einstein's energy momentum relation and thus can have heavier masses than the initial and final state particles.
Regarding the \B mixing, it is dominated by the top quark with a mass of $m_t = 173.3 \gev$ in the loop though the \B mass is only $m_\B = 5.3\gev$.
Thus, by a precise measurement of the \B meson mixing, one is sensitive to a particle with a mass more than 30 times higher than the \B mass and hence the necessary centre-of-mass energy needed for its production.
If there exists new heavy particles they might also contribute to such loop processes.
By a precise measurement of those, \lhcb tries to find deviations from Standard Model predictions for some variables.
Two of them, \asld and \Vub will be introduced in the following.
\begin{figure}[tb]
    \centering
	\includegraphics[width=0.6\textwidth]{bmixing}	
	\caption{One possible Feynman diagram for \B/\Bbar mixing. The \quark denotes either a \dquark or an \squark quark. Figure taken from \cite{LHCb_roadmap}.}
	\label{fig:Bmixing}
\end{figure}

\subsection{Flavour physics and the measurement of $|\Vub|$}
The flavour eigenstates of the weak interaction \ket{\dquark'},\ket{\squark'}, \ket{\bquark'} are not equal to the mass eigenstates \ket{\dquark},\ket{\squark},  \ket{\bquark}.
The transformation between those eigenstates is described by the unitary \textsc{Cabibbo-Kobayashi-Maskawa (CKM) matrix} $V_\text{CKM}$ \cite{Kobayashi_CKM}:
\begin{align}
\begin{pmatrix}
\ket{d'} \\ \ket{s'} \\ \ket{b'}
\end{pmatrix}
=
\begin{pmatrix}
V_{ud} & V_{us} & V_{ub} \\
V_{cd} & V_{cs} & V_{cb} \\
V_{td} & V_{ts} & V_{tb} \\
\end{pmatrix}
\cdot
\begin{pmatrix}
\ket{d} \\ \ket{s} \\ \ket{b}
\end{pmatrix}.
\end{align}
The probability that a quark with a mass eigenstate \ket{j} decays into a quark with mass eigenstate \ket{i} is given by $|V_{ij}|^2$.
As the CKM matrix is complex, there are in principle 18 free parameters.
Due to unitarity constraints and unobservable quark phases this number reduces to 4 parameters, which can be measured.
A convenient way is to write the CKM matrix in \textsc{Wolfenstein parametrisation} \cite{Wolfenstein}:
\begin{align}
    V_{\text{CKM}}=
    \begin{pmatrix}
    V_{ud} & V_{us} & V_{ub} \\
    V_{cd} & V_{cs} & V_{cb} \\
    V_{td} & V_{ts} & V_{tb} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
    1-\frac{\lambda^2}{2} & \lambda & A\lambda^3(\rho-\im\eta) \\
    -\lambda & 1-\frac{\lambda^2}{2} & A\lambda^2 \\
    A\lambda^3(1-\rho-\im\eta) & -A\lambda^2 & 1
    \end{pmatrix}
    + \mathcal{O}(\lambda^4)
\end{align}
With a value of $\lambda \approx 0.23$ \cite{PDG}, the Wolfenstein parametrisation clearly shows the almost diagonal structure of the CKM matrix, i.e. that quark transitions happen mostly within a generation.
The values of the CKM matrix elements are fundamental parameters of the Standard Model, thus their precise measurement and knowledge is important for the search for New Physics.
The unitarity of the CKM matrix imposes
\begin{align}
    \sum_j = V_{ij}V_{kj}^{\ast} = \delta_{ik}, \qquad \sum_i = V_{ij}V_{ik}^{\ast} = \delta_{jk}.
\end{align}
The vanishing combinations can be represented as triangles in a complex plane.
Most commonly one uses
\begin{align}
    \Vud\Vub^{\ast} + \Vcd\Vcb^{*} + \Vtd\Vtb^{\ast} = 0. \label{eq:triangle}
\end{align}
After division of equation (\ref{eq:triangle}) by $\Vcd\Vcd^{\ast}$, the vertices of the triangle are $(0,0), (0,1)$ and $(\bar{\rho}, \bar{\eta})$ with $\bar{\rho} = \rho (1 - \frac{\lambda^2}{2} + \ldots)$ and $\bar{\eta} = \eta (1 - \frac{\lambda^2}{2} + \ldots)$ as shown on the top part of Figure \ref{fig:CKM_triangle}.

The aim of flavour physics is to overconstrain the CKM matrix elements.
Many loop processes provide the ability to measure the absolute values of $V_{ij}$ or related quantities like the angles $\alpha, \beta, \gamma$ of the triangle and are furthermore sensitive to New Physics as explained above.
A lot of these results can be displayed and compared in the $(\bar{\rho}, \bar{\eta})$ plane as can be seen on the bottom part of Figure \ref{fig:CKM_triangle}.
A clear sign for New Physics would be for instance, that the triangle is not closed at the vertex of angle $\alpha$.
This is equivalent to the violation of unitarity of the CKM matrix \cite{PDG, Nierste}.

Currently, there is a big interest in the measurement of $|\Vub|$ since it is the least known among the CKM matrix element and there is an about $3\sigma$ deviation between measurements of $|\Vub|$ using either exclusive, \decay{\B}{\pi\ell\neul}, or inclusive, \decay{\B}{X_u\ell\neul}, semileptonic decays.
Theorists try to explain this discrepancy e.g. with the presence of a right-handed coupling of the $W$ boson as extension to the Standard Model \cite{Vub_RightHanded}.
A recent \lhcb study has measured $|\Vub|$ for the first time in a baryonic decay, namely \decay{\Lb}{\proton\mun\neumb} and confirmed this incompatibility \cite{SL_Vub}.
The decay \LbToDpmunuX, which is studied in this thesis, is a background in the latter $|\Vub|$ measurement.
The pollution of \decay{\Lb}{\proton\mun\neumb} with \LbToDpmunuX could only be estimated with simulation.
Thus a better understanding of \LbToDpmunuX could help to improve the determination of $|\Vub|$.
\begin{figure}[tb]
    \centering
	\includegraphics[width=0.49\textwidth]{ckm_triangle} \\	
	\includegraphics[width=\textwidth]{ckm_fit}	
	\caption{Top: Triangle in the complex $(\bar{\rho}, \bar{\eta})$ plane obtained by the unitarity constraint on the CKM matrix in equation (\ref{eq:triangle}). Bottom: Current status of the CKM matrix with all measured variables included. Figures taken from \cite{PDG} and \cite{CKM_fitter}
    respectively.}
	\label{fig:CKM_triangle}
\end{figure}

\subsection{The measurement of \CP violation in mixing with \asld}
The symmetry transformation \CP describes a combination of two symmetries:
The charge conjugation $\mathcal{C}$ transforms a particle into its antiparticle and the parity operation $\mathcal{P}$ inverts spatial coordinates.
If \CP were an exact symmetry, the physical laws would be the same for matter and antimatter.
Thus, the creation of our universe requires a violation of the \CP symmetry, otherwise all matter would have been annihilated with antimatter\footnote{This requirement is also known as one of the three Sakharov conditions for baryogenesis \cite{Sakharov}.}.
In the Standard Model, only the weak interaction breaks \CP symmetry.
Described by only one parameter, it appears as a single, complex phase in the CKM matrix\footnote{Without \CP violation the unitarity triangle would have a vanishing area.} \cite{Kobayashi_CKM}.
However, the \CP violation predicted by the Standard Model is too small to explain the observed matter in the universe.
Thus, the measurement of \CP violating observables may reveal New Physics contributions by deviations from the Standard Model predictions.

In principle one distinguishes three different kinds of \CP violation \cite{PDG, Nierste}:
\begin{enumerate}
    \item \textbf{\CP violation in decay} \\
          Let $M$ denote a charged or neutral hadron and $\overline{M}$ its \CP conjugate.
          Their decay amplitudes into a multi-particle final state $f$ or into the \CP conjugated state $\overline{f}$ respectively are defined as
          \begin{align}
              A_f := \bra{f}\Ham\ket{M}, \qquad \overline{A}_f := \bra{f}\Ham\ket{\overline{M}}, \nonumber \\
              A_{\overline{f}} := \bra{\overline{f}}\Ham\ket{M}, \qquad \overline{A}_{\overline{f}} := \bra{\overline{f}}\Ham\ket{\overline{M}},
          \end{align}
          where \Ham denotes the Hamiltonian governing weak interactions.
          Then, \CP violation in decay is defined by
          \begin{align}
            \left|\frac{\overline{A}_{\overline{f}}}{A_f}\right| \neq 1,
          \end{align}
          i.e. the probability that $\overline{M}$ decays into $\overline{f}$ is different from the probability, that $M$ decays into $f$.
    \item \textbf{\CP violation in mixing} \\
          A neutral meson, e.g. the \Bz meson can oscillate into its antiparticle \Bzb and this \Bzb again back into a \Bz.
          This phenomenon is also called \textsc{Neutral Meson Mixing} and a possible Feynman diagram of the \Bz/\Bzb is given in Figure \ref{fig:Bmixing}. 
          The time evolution of the flavour eigenstates \ket{\Bd} and \ket{\Bdb} can be phenomenologically described by an effective Schroedinger equation
          \begin{align}
              \im \diff{}{t}\begin{pmatrix} \ket{\text{\Bd}\vphantom{\text{\Bdb}}} \\ \ket{\text{\Bdb}} \end{pmatrix} = \left(M - \frac{\im}{2} \Gamma\right) \begin{pmatrix} \ket{\text{\Bd}\vphantom{\text{\Bdb}}} \\ \ket{\text{\Bdb}} \end{pmatrix},
          \end{align}
          where $M$ is the mass matrix and $\Gamma$ the decay matrix.
          Both, $M$ and $\Gamma$ are hermitian but not $\Ham = M - \frac{\im}{2} \Gamma$ due to the possible decay of the mesons.
          Due to the mixing, it is obvious that \Ham is not diagonal.
          After diagonalisation, the mass eigenstates \ket{B_H} and \ket{B_L} can be written in terms of the flavour eigenstates as
          \begin{align}
              \ket{B_{\text{H}}\vphantom{\text{\Bdb}}} &= p \ket{\text{\Bd}\vphantom{\text{\Bdb}}} - q \ket{\text{\Bdb}} \label{eq:b_heavy}\\ 
              \ket{B_{\text{L}}\vphantom{\text{\Bdb}}} &= p \ket{\text{\Bd}\vphantom{\text{\Bdb}}} + q \ket{\text{\Bdb}}, \qquad \text{with} \quad |p|^2 + |q|^2 = 1, \label{eq:b_light}
          \end{align}
          where the index H (heavy) denotes the mass state with the larger mass compared to the lighter one, indexed with L (light).
          The coefficients $p$ and $q$ can be related to the mass and decay matrix elements $M_{ij}$ respectively $\Gamma_{ij}$, but a thorough discussion of the phenomenology of mixing is outside the scope of this thesis.
          \CP violation in mixing means that the probability that a \Bd oscillates into a \Bdb is different from the probability that a \Bdb oscillates into a \Bd or formally
          \begin{align}
              \left|\frac{q}{p}\right| \neq 1.
          \end{align}
    \item \textbf{\CP violation in interference} \\
          If there exist final states $f$, that can be reached from a hadron $M$ as well as from its \CP conjugate $\overline{M}$, \CP violation can occur in the interference between a decay without mixing, $M \to f$, and a decay with mixing, $M \to \overline{M} \to f$.
          This kind of \CP violation is formally expressed by
          \begin{align}
              \Imag(\lambda_f) \neq 0,
          \end{align}
          where $\lambda_f$ is defined as
          \begin{align}
              \lambda_f := \frac{q}{p} \frac{\overline{A}_f}{A_f}.
          \end{align}
          As an example for \CP violation in mixing serves the decay \decay{\Bd/\Bdb}{\jpsi\KS}.
\end{enumerate}

For the measurement of \CP violation it is crucial to know the meson's flavour at decay, for instance if it was a \Bd or a \Bdb meson.
A common way to distinguish a \Bd or respectively a \Bdb decay is to use semileptonic decays, since the charge of the lepton contains the information about the meson's flavour:
The \bquark quark in the \Bd meson can only decay via the emission of a \Wm boson resulting in a negatively charged lepton (and a neutrino), whereas the \bquarkbar quark of the \Bdb emits a \Wp when decaying.
To measure \CP violation in \Bd/\Bdb mixing one defines the \textsc{Semileptonic Asymmetry} \asld as
\begin{align}
    \asld = \frac{P(\Bdb \to \Bd) - P(\Bd \to \Bdb)}{P(\Bdb \to \Bd) + P(\Bd \to \Bdb)},
\end{align}
where $P$ denotes the probability for the transition given in brackets.
The Standard Model prediction of $\asld = -(4.1 \pm 0.6) 10^{-4}$ \cite{asld_SM} is tiny compared to the current experimental sensitivity.
Thus, the measurement of \asld is an excellent test of the Standard Model and significant deviations from zero would be a signal of New Physics.
A current \lhcb measurement of \asld uses the decays \decay{\Bz}{\Dm\mup\neum X} as well as \decay{\Bz}{\Dstarm\mup\neum X}.
The result is $\asld = -(0.02 \pm 0.19\stat \pm 0.30\syst)\%$ and thus consistent with zero at the current precision \cite{asld_LHCb}.
With a contribution of $7\cdot 10^{-4}$, backgrounds coming from \Lb baryon decays make up around $23\%$ of the systematic error. As many \Lb decays have not been measured yet, they had to rely on rough estimates.
One of these backgrounds is the decay \LbToDpmunuX, which is studied in this thesis.
If one misses the proton and randomly adds a pion to the \Dz meson to form a \Dstar candidate, it fakes a \decay{\Bz}{\Dstarm\mup\neum X} decay.
Thus, the understanding of \LbToDpmunuX helps to improve the measurement of \asld by reducing the systematic uncertainties.

\section{Methods of parameter estimation}
This section briefly describes two methods to estimate parameters, which are used in this thesis.

\subsection{Maximum-Likelihood method}
A common task in High Energy Physics is to find the best estimate for a set of parameters $\vec{\theta}$ by a measurement of some variables $\vec{x}$.
As an example one wants to measure the mass $m_X$  and the width $\Gamma_X$ of a particle $X$ decaying into two daughter particles $A$ and $B$, i.e. $X \to AB$.
For this purpose one would measure the energies $E$ and momenta $p$ of the particles $A$ and $B$ multiple times.
Thus, the set of measured variables would be $\vec{x} = (E_A, \vec{p}_A, E_B, \vec{p}_B)$ and one would try to find the ``best" value for the parameters $\vec{\theta} = (m_X, \Gamma_X)$. 
The most frequently used method for the estimate of $\vec{\theta}$ is the \textsc{Maximum-Likelihood Method}.
Given a theoretical prediction of the measured distribution in form of a probability density function $f(\vec{x}|\vec{\theta})$ one can define the likelihood function
\begin{align}
    \mathcal{L}(\vec{x}|\vec{\theta}) := \prod_{i=1}^{N} f(\vec{x}_i|\vec{\theta}),
\end{align}
where $N$ denotes the number of (independent) measurements.
The maximum of this likelihood function $\mathcal{L}$ with respect to $\vec{\theta}$ is assumed to be the best estimate of $\vec{\theta}$.
Practically one minimises equivalently $-\log(\mathcal{L})$ for computational reasons.

Often, a probability density function $P$ is a linear combination of several components, e.g. a signal and background component $P_\text{sig}$ respectively $P_\text{bkg}$.
Thus, the number of events $N$ is a random variable as well.
If $N$ obeys the Poisson distribution, the so-called \textsc{Extended Likelihood Function} can be defined as: 
\begin{align}
    &\mathcal{L}_\text{ext}(\vec{x} | \Nsig, \Nbkg, \vec{\theta}) :=  \nonumber \\ 
    &\qquad \frac{(\Nsig+\Nbkg)^N\exp{(\Nsig+\Nbkg)}}{N!} \prod_{i=1}^{N} \left[f_\text{sig}P_\text{bkg}(\vec{x}_i|\vec{\theta}) + f_\text{bkg}P_\text{bkg}(\vec{x}_i|\vec{\theta})\right],
\end{align} 
where $N_j$ denotes the corresponding yield and $f_j:=\frac{N_j}{N}$ the fraction of the signal respectively background component.
Thus, the maximisation of the extended likelihood function $\mathcal{L}_\text{ext}$ enables to estimate the yields of each component at the same time \cite{Lista_Statistics, PDG}.

\subsection{Beeston-Barlow method}
\label{sec:BeestonBarlow}
When one tries to estimate the composition of a data sample, it might happen, that there does not exist an analytic form of the distribution.
Thus, one relies on simulations and has to bin the data into $n$ bins.
This gives a set of numbers ${d_i}$ with $i \in [1,n]$, where $d_i$ denotes the number of data events in falling into bin $i$.
Let $j$ denote a source contained in the data, $P_j$ its strength and $a_{ji}$ the number of simulated events from source $j$ in bin $i$, then the predicted number of events in bin $i$ is given by
\begin{align}
    f_i := N_\text{D} \sum_{j=1}^{m} \frac{P_j a_{ji}}{N_j},
\end{align}
where $N_\text{D}$ is the total number of events in the data sample, $N_j$ the number of simulated events for source $j$ and $m$ the number of sources.
Starting from a Poisson distributed probability of observing a particular $d_i$ as
\begin{align}
    \mathrm{e}^{f_i} \frac{f_i^{d_i}}{d_i!}
\end{align}
the logarithmic likelihood to be maximised looks like
\begin{align}
    \log \mathcal{L} = \sum_{i=1}^{n} \left[d_i \log(f_i) - f_i\right] \label{eq:BinnedLL}
\end{align}
after dropping the constant factorials.
This likelihood function is also known as \textsc{Binned Likelihood}.

Nonetheless, the binned likelihood of equation (\ref{eq:BinnedLL}) does not account for finite sizes of the simulation samples.
Due to large computation times simulation samples are often small and there are non-negligible statistical fluctuations in the $a_{ji}$.
Thus the likelihood function has to be modified as follows:
The predicted number of events in a bin is now
\begin{align}
    f_i := N_\text{D} \sum_{j=1}^{m} \frac{P_j A_{ji}}{N_j},
\end{align}
where $A_{ji}$ is the unknown expected number of events for source $j$ in bin $i$.
The ``observed" $a_{ji}$ in the simulation sample is generated from $A_{ji}$ by a Poisson distribution\footnote{Actually, the $a_{ji}$ obey a binomial distribution. However, for $A_{ji} << N_j$ it can be approximated by a Poisson distribution.}.
Thus, the probabilities of observing a set of ${d_i}$ and $a_{ji}$ have to be combined and the likelihood function to be maximised is
\begin{align}
    \log \mathcal{L} = \sum_{i=1}^{n} \left[d_i \log(f_i) - f_i\right] + \sum_{i=1}^{n} \sum_{j=1}^{m} \left[a_{ji} \log(A_{ji}) - A_{ji}\right]. \label{eq:BBLL}
\end{align}
Throughout this analysis, the maximisation of this likelihood function in equation (\ref{eq:BBLL}) is referred to as \textsc{Beeston-Barlow Method} \cite{BeestonBarlow}.
